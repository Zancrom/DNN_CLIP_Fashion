{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maui/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "import tqdm\n",
    "\n",
    "from torchvision import models\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion Product Image Dataset: https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>usage</th>\n",
       "      <th>productDisplayName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15970</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39386</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Peter England Men Party Blue Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59263</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Titan Women Silver Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21379</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Track Pants</td>\n",
       "      <td>Black</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Manchester United Men Solid Black Track Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53759</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Grey T-shirt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id gender masterCategory subCategory  articleType baseColour  season  \\\n",
       "0  15970    Men        Apparel     Topwear       Shirts  Navy Blue    Fall   \n",
       "1  39386    Men        Apparel  Bottomwear        Jeans       Blue  Summer   \n",
       "2  59263  Women    Accessories     Watches      Watches     Silver  Winter   \n",
       "3  21379    Men        Apparel  Bottomwear  Track Pants      Black    Fall   \n",
       "4  53759    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n",
       "\n",
       "     year   usage                             productDisplayName  \n",
       "0  2011.0  Casual               Turtle Check Men Navy Blue Shirt  \n",
       "1  2012.0  Casual             Peter England Men Party Blue Jeans  \n",
       "2  2016.0  Casual                       Titan Women Silver Watch  \n",
       "3  2011.0  Casual  Manchester United Men Solid Black Track Pants  \n",
       "4  2012.0  Casual                          Puma Men Grey T-shirt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data (the csv file has some bad lines)\n",
    "df = pd.read_csv('data/myntradataset/styles.csv', on_bad_lines='skip')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([3677, 14291, 17639, 22089, 23484, 37531, 40675], dtype='int64')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the rows with missing values\n",
    "nan_values = df[df['productDisplayName'].isnull()].index\n",
    "df = df.dropna(subset=['productDisplayName'])\n",
    "\n",
    "nan_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maui/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7a7e61a4a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images = []\n",
    "\n",
    "for img in os.listdir('data/myntradataset/images/'):\n",
    "    if img.endswith('.jpg'):\n",
    "        id_ = int(img.split('.')[0])\n",
    "        \n",
    "        # Check if the image is in the dataframe\n",
    "        if not df[df['id'] == id_].empty and id_ not in nan_values:\n",
    "            image = cv2.imread('data/myntradataset/images/' + img)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            images.append(image)\n",
    "\n",
    "idx = np.random.randint(0, len(images))\n",
    "plt.imshow(images[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@20.635] global loadsave.cpp:241 findDecoder imread_('data/myntradataset/images/39403.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@21.070] global loadsave.cpp:241 findDecoder imread_('data/myntradataset/images/39410.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@21.817] global loadsave.cpp:241 findDecoder imread_('data/myntradataset/images/39401.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@22.002] global loadsave.cpp:241 findDecoder imread_('data/myntradataset/images/39425.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@22.169] global loadsave.cpp:241 findDecoder imread_('data/myntradataset/images/12347.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[39403, 39410, 39401, 39425, 12347]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_ = df['id'].values\n",
    "no_images = []\n",
    "\n",
    "# Ids without images\n",
    "for id_ in ids_:\n",
    "    img = cv2.imread('data/myntradataset/images/' + str(id_) + '.jpg')\n",
    "    if img is None:\n",
    "        no_images.append(id_)\n",
    "\n",
    "no_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def preprocess_text(text):\n",
    "    return tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, train=True, transform=None):\n",
    "        self.data = pd.read_csv(csv_file, on_bad_lines='skip')\n",
    "        self.data = self.data.dropna(subset=['productDisplayName'])\n",
    "        self.data = self.data[~self.data['id'].isin(no_images)]\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "\n",
    "        train_data, test_data = train_test_split(self.data, test_size=0.2, random_state=42)\n",
    "        self.data = train_data if self.train else test_data\n",
    "\n",
    "        self.labels = preprocess_text(self.data['productDisplayName'].astype(str).values.tolist())['input_ids']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, str(self.data.iloc[idx, 0]) + \".jpg\")\n",
    "        image = cv2.imread(img_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "        image = image.reshape(3, 224, 224)\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 1111\n",
      "Test dataset size: 278\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = DataLoader(FashionDataset('data/myntradataset/styles.csv', 'data/myntradataset/images/', train=True, transform=transform), batch_size=32, shuffle=True)\n",
    "test_dataset = DataLoader(FashionDataset('data/myntradataset/styles.csv', 'data/myntradataset/images/', train=False, transform=transform), batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.fc = nn.Linear(768, 512)  # BERT outputs 768-dim embeddings\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return self.fc(output.pooler_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.model = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Freeze the model\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(image_embeddings, text_embeddings, temperature=0.07):\n",
    "    image_embeddings = nn.functional.normalize(image_embeddings, dim=1)\n",
    "    text_embeddings = nn.functional.normalize(text_embeddings, dim=1)\n",
    "\n",
    "    logits = torch.mm(image_embeddings, text_embeddings.t()) / temperature\n",
    "    labels = torch.arange(len(image_embeddings)).to(image_embeddings.device)\n",
    "    \n",
    "    return nn.CrossEntropyLoss()(logits, labels) + nn.CrossEntropyLoss()(logits.t(), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(text_encoder, image_encoder, dataloader, optimizer, device, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        text_encoder.train()\n",
    "        image_encoder.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, input_ids in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            text_embeddings = text_encoder(input_ids, attention_mask)\n",
    "            image_embeddings = image_encoder(images)\n",
    "\n",
    "            loss = contrastive_loss(image_embeddings, text_embeddings)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maui/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/maui/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "image_model = ImageEncoder().to(device)\n",
    "text_model = TextEncoder().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(image_model.parameters()) + list(text_model.parameters()), lr=0.001)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train(text_model, image_model, train_dataset, optimizer, device, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our models\n",
    "torch.save(image_model.state_dict(), 'image_model.pth')\n",
    "torch.save(text_model.state_dict(), 'text_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def test_clip_model(text_encoder, image_encoder, dataloader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the CLIP model on a test dataset.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained CLIP model.\n",
    "        test_dataloader (DataLoader): DataLoader for the test dataset.\n",
    "        device (torch.device): The device to run the evaluation on (CPU or GPU).\n",
    "    \n",
    "    Returns:\n",
    "        float: The average similarity score for matching image-text pairs.\n",
    "    \"\"\"\n",
    "    text_encoder.eval()\n",
    "    image_encoder.eval()\n",
    "    total_similarity = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, input_ids in dataloader:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            \n",
    "            text_embeddings = text_encoder(input_ids, attention_mask)\n",
    "            image_embeddings = image_encoder(images)\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            similarity = cosine_similarity(image_embeddings, text_embeddings, dim=1)\n",
    "            \n",
    "            # Accumulate results\n",
    "            total_similarity += similarity.sum().item()\n",
    "            count += len(similarity)\n",
    "    \n",
    "    # Compute average similarity\n",
    "    average_similarity = total_similarity / count\n",
    "    return average_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.03581682660484937"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clip_model(text_model, image_model, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(text_encoder, image_encoder, dataloader, device):\n",
    "        total_text_embeddings = []\n",
    "        total_image_embeddings = []\n",
    "        for images, input_ids in dataloader:\n",
    "                attention_mask = torch.ones_like(input_ids)\n",
    "                images = images.to(device)\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                \n",
    "                text_embeddings = text_encoder(input_ids, attention_mask)\n",
    "                image_embeddings = image_encoder(images)\n",
    "\n",
    "                total_text_embeddings += text_embeddings\n",
    "                total_image_embeddings += image_embeddings\n",
    "        return text_embeddings, image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 12.53 GiB is allocated by PyTorch, and 53.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35058/2968722410.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_35058/74593255.py\u001b[0m in \u001b[0;36mget_embeddings\u001b[0;34m(text_encoder, image_encoder, dataloader, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mtext_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mimage_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35058/3198763547.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    986\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         )\n\u001b[0;32m--> 988\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    989\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 )\n\u001b[1;32m    581\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    583\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    473\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 402\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 12.53 GiB is allocated by PyTorch, and 53.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "text_embeddings, image_embeddings = get_embeddings(text_model, image_model, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2792493402957916,\n",
       " 0.27886611223220825,\n",
       " 0.2808966636657715,\n",
       " 0.2803189754486084,\n",
       " 0.2793749272823334,\n",
       " 0.279426246881485,\n",
       " 0.2785947024822235,\n",
       " 0.27934443950653076,\n",
       " 0.27933865785598755,\n",
       " 0.2785298824310303,\n",
       " 0.2791574001312256,\n",
       " 0.27887430787086487,\n",
       " 0.278449147939682,\n",
       " 0.2785258889198303,\n",
       " 0.2792377769947052,\n",
       " 0.27958792448043823,\n",
       " 0.2796388268470764,\n",
       " 0.27805638313293457,\n",
       " 0.28023993968963623]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(text_embeddings))\n",
    "\n",
    "sim = []\n",
    "img_idx = []\n",
    "\n",
    "for i, img_embedding in enumerate(image_embeddings):\n",
    "    img_idx.append(i)\n",
    "    similarity = cosine_similarity(img_embedding.unsqueeze(0), text_embeddings[idx].unsqueeze(0), dim=1)\n",
    "    sim.append(np.abs(similarity.item()))\n",
    "\n",
    "sim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
