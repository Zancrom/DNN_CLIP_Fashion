{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb1zi5YTKQug"
      },
      "source": [
        "# CLIP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip data/archive\\ \\(2\\).zip -d data/"
      ],
      "metadata": {
        "id": "qE0N6zcoNVfi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XmbznRJ7KQuh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.functional as F\n",
        "import tqdm\n",
        "\n",
        "from torchvision import models\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k64rr0z-KQuh"
      },
      "source": [
        "Fashion Product Image Dataset: https://www.kaggle.com/datasets/paramaggarwal/fashion-product-images-small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qjHH_M85KQuh",
        "outputId": "79e384a6-4ce2-4d2c-f090-00d4691699d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id gender masterCategory subCategory  articleType baseColour  season  \\\n",
              "0  15970    Men        Apparel     Topwear       Shirts  Navy Blue    Fall   \n",
              "1  39386    Men        Apparel  Bottomwear        Jeans       Blue  Summer   \n",
              "2  59263  Women    Accessories     Watches      Watches     Silver  Winter   \n",
              "3  21379    Men        Apparel  Bottomwear  Track Pants      Black    Fall   \n",
              "4  53759    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n",
              "\n",
              "     year   usage                             productDisplayName  \n",
              "0  2011.0  Casual               Turtle Check Men Navy Blue Shirt  \n",
              "1  2012.0  Casual             Peter England Men Party Blue Jeans  \n",
              "2  2016.0  Casual                       Titan Women Silver Watch  \n",
              "3  2011.0  Casual  Manchester United Men Solid Black Track Pants  \n",
              "4  2012.0  Casual                          Puma Men Grey T-shirt  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9798edf1-965e-4789-9474-141a0bca0ed6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>masterCategory</th>\n",
              "      <th>subCategory</th>\n",
              "      <th>articleType</th>\n",
              "      <th>baseColour</th>\n",
              "      <th>season</th>\n",
              "      <th>year</th>\n",
              "      <th>usage</th>\n",
              "      <th>productDisplayName</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15970</td>\n",
              "      <td>Men</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Topwear</td>\n",
              "      <td>Shirts</td>\n",
              "      <td>Navy Blue</td>\n",
              "      <td>Fall</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39386</td>\n",
              "      <td>Men</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Bottomwear</td>\n",
              "      <td>Jeans</td>\n",
              "      <td>Blue</td>\n",
              "      <td>Summer</td>\n",
              "      <td>2012.0</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Peter England Men Party Blue Jeans</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>59263</td>\n",
              "      <td>Women</td>\n",
              "      <td>Accessories</td>\n",
              "      <td>Watches</td>\n",
              "      <td>Watches</td>\n",
              "      <td>Silver</td>\n",
              "      <td>Winter</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Titan Women Silver Watch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>21379</td>\n",
              "      <td>Men</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Bottomwear</td>\n",
              "      <td>Track Pants</td>\n",
              "      <td>Black</td>\n",
              "      <td>Fall</td>\n",
              "      <td>2011.0</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Manchester United Men Solid Black Track Pants</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>53759</td>\n",
              "      <td>Men</td>\n",
              "      <td>Apparel</td>\n",
              "      <td>Topwear</td>\n",
              "      <td>Tshirts</td>\n",
              "      <td>Grey</td>\n",
              "      <td>Summer</td>\n",
              "      <td>2012.0</td>\n",
              "      <td>Casual</td>\n",
              "      <td>Puma Men Grey T-shirt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9798edf1-965e-4789-9474-141a0bca0ed6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9798edf1-965e-4789-9474-141a0bca0ed6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9798edf1-965e-4789-9474-141a0bca0ed6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a059e9ba-e7b4-47a5-b5d9-522e489777be\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a059e9ba-e7b4-47a5-b5d9-522e489777be')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a059e9ba-e7b4-47a5-b5d9-522e489777be button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 44424,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17049,\n        \"min\": 1163,\n        \"max\": 60000,\n        \"num_unique_values\": 44424,\n        \"samples\": [\n          12532,\n          3469,\n          24742\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Women\",\n          \"Unisex\",\n          \"Boys\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"masterCategory\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Apparel\",\n          \"Accessories\",\n          \"Sporting Goods\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subCategory\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 45,\n        \"samples\": [\n          \"Hair\",\n          \"Makeup\",\n          \"Free Gifts\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"articleType\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 143,\n        \"samples\": [\n          \"Hair Colour\",\n          \"Flats\",\n          \"Lip Care\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"baseColour\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 46,\n        \"samples\": [\n          \"Turquoise Blue\",\n          \"Multi\",\n          \"Magenta\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"season\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Summer\",\n          \"Spring\",\n          \"Fall\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.1264802098149014,\n        \"min\": 2007.0,\n        \"max\": 2019.0,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          2009.0,\n          2019.0,\n          2011.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"usage\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Ethnic\",\n          \"Travel\",\n          \"Casual\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"productDisplayName\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 31121,\n        \"samples\": [\n          \"Wildcraft Unisex Gear for Life Yellow Backpack\",\n          \"Nike Unisex Hayward 25m B Black Backpacks\",\n          \"ADIDAS Men Badge P/O Black Sweaters\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data (the csv file has some bad lines)\n",
        "df = pd.read_csv('data/myntradataset/styles.csv', on_bad_lines='skip')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RiacDuz_KQui",
        "outputId": "6bc77c14-3fc6-4342-b971-5716a860b848",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([3677, 14291, 17639, 22089, 23484, 37531, 40675], dtype='int64')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Drop the rows with missing values\n",
        "nan_values = df[df['productDisplayName'].isnull()].index\n",
        "df = df.dropna(subset=['productDisplayName'])\n",
        "\n",
        "nan_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WlObFyARKQui",
        "outputId": "f0cb9751-7668-474f-a078-a1cb63bd3c54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7b87060ffb50>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "images = []\n",
        "\n",
        "for img in os.listdir('data/myntradataset/images/'):\n",
        "    if img.endswith('.jpg'):\n",
        "        id_ = int(img.split('.')[0])\n",
        "\n",
        "        # Check if the image is in the dataframe\n",
        "        if not df[df['id'] == id_].empty and id_ not in nan_values:\n",
        "            image = cv2.imread('data/myntradataset/images/' + img)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            images.append(image)\n",
        "\n",
        "idx = np.random.randint(0, len(images))\n",
        "plt.imshow(images[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2P2RGlBnKQui",
        "outputId": "62395f95-c922-40c5-d033-f4b0e009a34c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[39403, 39410, 39401, 39425, 12347]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "ids_ = df['id'].values\n",
        "no_images = []\n",
        "\n",
        "# Ids without images\n",
        "for id_ in ids_:\n",
        "    img = cv2.imread('data/myntradataset/images/' + str(id_) + '.jpg')\n",
        "    if img is None:\n",
        "        no_images.append(id_)\n",
        "\n",
        "no_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K8KXor1jKQui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839b040d-6718-4603-e32b-c6f8587ca954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "def preprocess_text(text):\n",
        "    return tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zLEfFpSKKQui"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, train=True, transform=None):\n",
        "        self.data = pd.read_csv(csv_file, on_bad_lines='skip')\n",
        "        self.data = self.data.dropna(subset=['productDisplayName'])\n",
        "        self.data = self.data[~self.data['id'].isin(no_images)]\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.train = train\n",
        "\n",
        "        train_data, test_data = train_test_split(self.data, test_size=0.2, random_state=42)\n",
        "        self.data = train_data if self.train else test_data\n",
        "\n",
        "        self.labels = preprocess_text(self.data['productDisplayName'].astype(str).values.tolist())['input_ids']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, str(self.data.iloc[idx, 0]) + \".jpg\")\n",
        "        image = cv2.imread(img_name)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (224, 224))\n",
        "        image = image.reshape(3, 224, 224)\n",
        "        image = torch.tensor(image, dtype=torch.float32)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "N-JAaBuzKQuj",
        "outputId": "ff1f38fe-135c-412d-b99e-d9be1393d2b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 1111\n",
            "Test dataset size: 278\n"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "])\n",
        "\n",
        "train_dataset = DataLoader(FashionDataset('data/myntradataset/styles.csv', 'data/myntradataset/images/', train=True, transform=transform), batch_size=32, shuffle=True)\n",
        "test_dataset = DataLoader(FashionDataset('data/myntradataset/styles.csv', 'data/myntradataset/images/', train=False, transform=transform), batch_size=32, shuffle=True)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nn_mJlD1KQuj"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.transformer = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.fc = nn.Linear(768, 512)  # BERT outputs 768-dim embeddings\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return self.fc(output.pooler_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0Imt79v7KQuj"
      },
      "outputs": [],
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        self.model = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Freeze the model\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, 512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RgG1KLBnKQuk"
      },
      "outputs": [],
      "source": [
        "def contrastive_loss(image_embeddings, text_embeddings, temperature=0.07):\n",
        "    image_embeddings = nn.functional.normalize(image_embeddings, dim=1)\n",
        "    text_embeddings = nn.functional.normalize(text_embeddings, dim=1)\n",
        "\n",
        "    logits = torch.mm(image_embeddings, text_embeddings.t()) / temperature\n",
        "    labels = torch.arange(len(image_embeddings)).to(image_embeddings.device)\n",
        "\n",
        "    return nn.CrossEntropyLoss()(logits, labels) + nn.CrossEntropyLoss()(logits.t(), labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "puci8jdDKQuk"
      },
      "outputs": [],
      "source": [
        "def train(text_encoder, image_encoder, dataloader, optimizer, device, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        text_encoder.train()\n",
        "        image_encoder.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for images, text in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            attention_mask = torch.ones_like(text)\n",
        "            images = images.to(device)\n",
        "            text = text.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            text_embeddings = text_encoder(text, attention_mask)\n",
        "            image_embeddings = image_encoder(images)\n",
        "\n",
        "            loss = contrastive_loss(image_embeddings, text_embeddings)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwQk84idKQuk",
        "outputId": "2b50d5ba-38d8-473e-c9b0-13972459b1b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1/10: 100%|| 1111/1111 [04:22<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 6.941022387503719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|| 1111/1111 [04:23<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Loss: 6.92920859325694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|| 1111/1111 [04:27<00:00,  4.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Loss: 6.929220061383733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|| 1111/1111 [04:25<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Loss: 6.929204568682652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|| 1111/1111 [04:27<00:00,  4.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Loss: 6.92920529102013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|| 1111/1111 [04:21<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Loss: 6.92920764344539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|| 1111/1111 [04:26<00:00,  4.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Loss: 6.929206150700443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|| 1111/1111 [04:21<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Loss: 6.935439956606668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|| 1111/1111 [04:26<00:00,  4.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Loss: 6.92919607231147\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10:  32%|      | 357/1111 [01:24<02:54,  4.33it/s]"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "image_model = ImageEncoder().to(device)\n",
        "text_model = TextEncoder().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(list(image_model.parameters()) + list(text_model.parameters()), lr=0.001)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train(text_model, image_model, train_dataset, optimizer, device, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHT-12V-KQuk"
      },
      "outputs": [],
      "source": [
        "# Save our models\n",
        "torch.save(image_model.state_dict(), 'image_model.pth')\n",
        "torch.save(text_model.state_dict(), 'text_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfTEP1TdKQuk"
      },
      "outputs": [],
      "source": [
        "from torch.nn.functional import cosine_similarity\n",
        "\n",
        "def test_clip_model(text_encoder, image_encoder, dataloader, device):\n",
        "    text_encoder.eval()\n",
        "    image_encoder.eval()\n",
        "    total_similarity = 0.0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, text in dataloader:\n",
        "            attention_mask = torch.ones_like(text)\n",
        "            images = images.to(device)\n",
        "            text = text.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "\n",
        "            text_embeddings = text_encoder(text, attention_mask)\n",
        "            image_embeddings = image_encoder(images)\n",
        "\n",
        "            # Compute cosine similarity\n",
        "            similarity = cosine_similarity(image_embeddings, text_embeddings, dim=1)\n",
        "\n",
        "            # Accumulate results\n",
        "            total_similarity += similarity.sum().item()\n",
        "            count += len(similarity)\n",
        "\n",
        "    # Compute average similarity\n",
        "    average_similarity = total_similarity / count\n",
        "    return average_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4th4QjMiKQul"
      },
      "outputs": [],
      "source": [
        "test_clip_model(text_model, image_model, test_dataset, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fS3ThEtOKQul"
      },
      "outputs": [],
      "source": [
        "# For a given text index give the k most similar image\n",
        "def get_most_similar_images(image_encoder, text_encoder, dataloader, text, device, top_k=5):\n",
        "    text_encoder.eval()\n",
        "    image_encoder.eval()\n",
        "    total_similarity = 0.0\n",
        "    count = 0\n",
        "\n",
        "    text = torch.stack([text])\n",
        "\n",
        "    attention_mask = torch.ones_like(text)\n",
        "    text = text.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    text_embeddings = text_encoder(text, attention_mask)\n",
        "\n",
        "    top_similar_images = []\n",
        "    top_similarities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, _ in dataloader:\n",
        "            images = images.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "\n",
        "            image_embeddings = image_encoder(images)\n",
        "\n",
        "            # Compute cosine similarity\n",
        "            similarity = cosine_similarity(image_embeddings, text_embeddings, dim=1)\n",
        "\n",
        "            for img, sim in zip(images, similarity):\n",
        "                sim = np.abs(sim.item())\n",
        "                if len(top_similar_images) < top_k:\n",
        "                    top_similar_images.append(img)\n",
        "                    top_similarities.append(sim.item())\n",
        "                else:\n",
        "                    if sim < top_similarities[-1]:\n",
        "                        top_similar_images[-1] = img\n",
        "\n",
        "                # Sort\n",
        "                top_similar_images, top_similarities = zip(*sorted(zip(top_similar_images, top_similarities), key=lambda x: x[1], reverse=True))\n",
        "                top_similar_images = list(top_similar_images)\n",
        "                top_similarities = list(top_similarities)\n",
        "\n",
        "    return top_similar_images, top_similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1noBpZ07KQul"
      },
      "outputs": [],
      "source": [
        "idx = np.random.randint(0, len(test_dataset))\n",
        "image, text = test_dataset.dataset[idx]\n",
        "\n",
        "k_similar_img, _ = get_most_similar_images(image_model, text_model, test_dataset, text, device, top_k=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_dataset.dataset.data.iloc[idx, 9])\n",
        "\n",
        "for img in k_similar_img:\n",
        "    img = img.reshape(224, 224, 3)\n",
        "    img = img.to('cpu').numpy()\n",
        "    plt.imshow(img.astype(np.uint8))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "sdFpDHBDnpjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For a given image index give the 5 most similar description\n",
        "def get_most_similar_texts(image_encoder, text_encoder, dataloader, image, device, top_k=5):\n",
        "    text_encoder.eval()\n",
        "    image_encoder.eval()\n",
        "    total_similarity = 0.0\n",
        "    count = 0\n",
        "\n",
        "    image = torch.stack([image])\n",
        "\n",
        "    image = image.to(device)\n",
        "    image_embeddings = image_encoder(image)\n",
        "\n",
        "    top_similar_texts = []\n",
        "    top_similarities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, texts in dataloader:\n",
        "            attention_mask = torch.ones_like(texts)\n",
        "            texts = texts.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "\n",
        "            text_embeddings = text_encoder(texts, attention_mask)\n",
        "\n",
        "            # Compute cosine similarity\n",
        "            similarity = cosine_similarity(image_embeddings, text_embeddings, dim=1)\n",
        "\n",
        "            for txt, sim in zip(texts, similarity):\n",
        "                sim = np.abs(sim.item())\n",
        "                if len(top_similar_texts) < top_k:\n",
        "                    top_similar_texts.append(txt)\n",
        "                    top_similarities.append(sim.item())\n",
        "                else:\n",
        "                    if sim < top_similarities[-1]:\n",
        "                        top_similar_texts[-1] = txt\n",
        "\n",
        "                # Sort\n",
        "                top_similar_texts, top_similarities = zip(*sorted(zip(top_similar_texts, top_similarities), key=lambda x: x[1], reverse=True))\n",
        "                top_similar_texts = list(top_similar_texts)\n",
        "                top_similarities = list(top_similarities)\n",
        "\n",
        "    return top_similar_texts, top_similarities"
      ],
      "metadata": {
        "id": "tMeM1W9SvAFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_similar_txt, _ = get_most_similar_texts(image_model, text_model, test_dataset, image, device, top_k=5)"
      ],
      "metadata": {
        "id": "yrqK4Lgwv1L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = image.reshape(224, 224, 3)\n",
        "img = img.to('cpu').numpy()\n",
        "plt.imshow(img.astype(np.uint8))\n",
        "plt.show()\n",
        "\n",
        "for txt in k_similar_txt:\n",
        "    print(tokenizer.decode(txt))"
      ],
      "metadata": {
        "id": "xvBvUXRSwRh6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}